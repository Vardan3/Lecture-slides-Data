---
title: "Lab 08 Regularization"
author: "Lusine Zilfimian"
date: |
     `r format(as.Date("2020-04-13"), '%B %d (%A),  %Y')`
fontsize: 9pt
output: 
    beamer_presentation:
      theme: "AnnArbor"
      colortheme: "beaver"
      fonttheme: "structurebold"
      fig_width: 3.5
      fig_height: 2.5
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = T)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(ggplot2, dplyr)
```

 ---
 
# Contents

 * Libraries
 * Data Preparation
 * Ridge
 * LASSO
 * Cross Validation 
 * Elastic Nets
 
 
 ---
 
# Needed packages

```{r message=FALSE, warning=FALSE}
library(ISLR) # for data Hitters
library(dplyr)
library(glmnet)
library(ggthemes)
```

 ---
 
## Ridge


* Understand the dataset
* Make training and testing sets


```{r}
colnames(Hitters)
#?Hitters
str(Hitters)
summary(Hitters)
Hitters <- na.omit(Hitters)
dim(Hitters)

set.seed(27)
index <- sample(nrow(Hitters), nrow(Hitters) * 0.2, replace = F)
train <- Hitters[index, 10:20]
test <- Hitters[-index, 10:20]
dim(train)
```

\pagebreak

 * Unlike the modeling syntax before, with glmnet() we need to 
   
   * separately define dependent variable (y)
   
   * set of independent variables in the form of a matrix (x)
   
   * With alpha=1 we will get lasso regressing, with alpha=0, we will get ridge regression
Set lambda, the penalizing parameter, equal to 0 


```{r}
x <- model.matrix(Salary ~ .,train)[,-1] 
y <- train$Salary

ridge.mod <- glmnet(x, y,lambda = 0, alpha = 0)
names(ridge.mod)

ridge.mod$lambda

ridge.mod$a0
ridge.mod$beta

# lambda *(alpha Lasso + (1-alpha) Ridge)
# inverse order of lambda
grid <- 1:5
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
ridge.mod
ridge.mod$lambda
```

glmnet stores all the coefficients for each model in order of **largest to smallest** lambda.

You can see how the largest lambda value has pushed these coefficients to nearly 0.

```{r}
grid <- 10 ^ seq(10,-2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid, standardize = T)
ridge.mod$a0
```

\pagebreak

```{r}
options(scipen = 999)
coef(ridge.mod)[c("CRuns", "CWalks"), 100]
coef(ridge.mod)[c("CRuns", "CWalks"), 1]
```

```{r}
as.matrix(ridge.mod$beta)[,1:3]
```

\pagebreak

```{r}
plot(ridge.mod, xvar="lambda")
legend("bottomright", lwd = 2, col = 1:10, legend = colnames(x), cex = .6)
```

 * Each curve corresponds to a variable.
 
 * As $\lambda$ increases, the ridge coefficient estimates shrink towards zero.
 
 * When $\lambda$ is extremely large, then all of the ridge coefficient estimates are basically zero; this corresponds to the null model that contains no predictors.
 
 * The axis above indicates the number of nonzero coefficients at the current $\lambda$.
 * In Ridge regression the number of variables remains the same.

 

\pagebreak
```{r}
plot(ridge.mod, xvar="dev")
legend("bottomleft", lwd = 2, col = 1:10, legend = colnames(x), cex = .6)
```
\pagebreak

```{r}
plot(ridge.mod$beta[4,], xlab = "Index of lambda", ylab = "Number of walks", type = "b" )

walk <- data.frame(y = ridge.mod$beta[4,])
library(ggthemes)
ggplot(walk, aes(x = 1:100, y = y)) + 
  geom_point(alpha = 0.5, size = 2, col = "seagreen4")+
  xlab("Index of lambda") +
  ylab("Number of walks") + 
  theme_pander()

which.max(coef(ridge.mod)[ "CWalks", ])
which.min(coef(ridge.mod)[ "CWalks", ])  
max(coef(ridge.mod)[ "CWalks", ])
min(coef(ridge.mod)[ "CWalks", ])
```

  To make predictions, we need to specify the value s, if s is numeric it shows the value of lambda used for regularization


```{r}
test_pr <- predict(ridge.mod, s = (0:500), newx = model.matrix(Salary ~ ., test)[,-1])

rmse <- data.frame(rmse = apply(test_pr, 2, 
  function (x) mean((x - test$Salary)^2 )))

ggplot(rmse, aes(x = 1:501, y = rmse)) + 
  geom_point(alpha = 0.5, size = 2, col = "mediumvioletred")+
  xlab("") +
  ylab("MSE") + 
  theme_pander()
```

\pagebreak
## Lasso


 * Create l1 vector with values for lambda
 * set alpha=1, to get LASSO regression
 * Highly correlated numeric variables:


```{r}
cor(train[,c(1:4,7:9,10)])
grid <- seq(from = 0, to = 3, by = 0.5)
lasso.mod <- glmnet(y = y, x = x, lambda = grid, alpha = 1, standardize = T)
lasso.mod
```
\pagebreak
```{r}
grid <- 2 ^ seq(8,-1, length = 100)
lasso.mod <- glmnet(y = y, x = x, lambda = grid, alpha = 1, standardize = T)
plot(lasso.mod, xvar="lambda")
legend("bottomright", lwd = 2, col = 1:10, legend = colnames(x), cex = .6)
```
\pagebreak
```{r}
plot(lasso.mod, xvar="dev")
legend("bottomleft", lwd = 2, col = 1:10, legend = colnames(x), cex = .6)
```


\pagebreak


```{r}
length(lasso.mod$lambda)
coeff <- as.matrix(lasso.mod$beta)
coeff[,1:6]
coeff[,c(14,25,50,70,80,90)]
coeff_non_zero <- coeff[coeff[,50]!=0,]
dim(train)
dim(coeff_non_zero)
rownames(coeff_non_zero)
```


With $\lambda_{50}$ we are getting 5 variables instead of 11 original variables


\pagebreak
### Cross-validation

 * Doing 10-fold cross validation 
 * 
 
```{r}
set.seed(2708)
grid <- exp(seq(-2,11, length = 100))
lasso.cv <- cv.glmnet(y = y, x = x, lambda = grid, alpha = 1, nfolds = 10)
names(lasso.cv)
lasso.cv$nzero # Number of non-zero variables for each lambda
```

The mean cross - validated error - MSE not RMSE

```{r}
lasso.cv$cvm
```


\pagebreak

Here are the subset of lambda values, mean square errors, standard errors of MSE and bounds for MSE:


```{r}
head(data.frame( Lambda = lasso.cv$lambda,
  CVM = lasso.cv$cvm, SD = lasso.cv$cvsd,
  CVUP = lasso.cv$cvup, CVLO = lasso.cv$cvlo
))
```

 * Finally, lambda values

```{r}
lasso.cv$lambda
```


\pagebreak

 * Cross-validation curve (red dotted line), and upper and lower standard deviation curves along:

 * $\lambda_{min}$ that gives a minimum mean cross-validated error
 *  $\lambda_{1se}$ gives the most regularized model such that error is within one standard error of the minimum


 * Lowest MSE is achieved when lambda = ?
```{r}
plot(lasso.cv)
```

\pagebreak

 * Lets build the model with the lambda for lowest MSE

```{r}
lasso.cv$lambda.min
lasso.mod1 <- glmnet(y = y, x = x, lambda=lasso.cv$lambda.min, alpha=1, standardize = T)
print(lasso.mod1)
lasso.cv$lambda.1se
```

## Elastic Net


 * Any alpha value between 0-1 will perform an elastic net.
 * When alpha = 0.5 we perform an equal combination of penalties whereas alpha → 0 will have a heavier ridge penalty applied and alpha → 1 will have a heavier lasso penalty


```{r}
lasso    <- glmnet(x, y, alpha = 1, standardize = T)
elastic1 <- glmnet(x, y, alpha = 0.25, standardize = T) 
elastic2 <- glmnet(x, y, alpha = 0.75, standardize = T) 
ridge    <- glmnet(x, y, alpha = 0, standardize = T)

plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n")

```

### Best El net

 * In elastic nets we want to tune both λ and the α parameters:

```{r}
elnet <- glmnet(x, y, alpha=.5, standardize = T)

models <- list()
set.seed(2708)
for (i in 0:10) {
  name <- paste0("alpha", i/10)
  models[[name]] <- cv.glmnet(x, y, type.measure="mse", alpha=i/10)
}
 
results <- data.frame()
xt <- model.matrix(Salary ~ .,test)[,-1] 
yt <- test$Salary

for (i in 0:10) {
  name <- paste0("alpha", i/10)
  predicted <- predict(models[[name]], s = models[[name]]$lambda.1se, newx = xt)
  mse <-mean((yt - predicted)^2)
  temp <- data.frame(alpha=i/10, mse = mse, name = name)
  results <- rbind(results, temp)
}
 
results
min(results$mse)
```
